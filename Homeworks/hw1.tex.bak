\documentclass[12pt]{article}
\usepackage{geometry,amsmath,amssymb, graphicx, natbib, float, enumerate}
\geometry{margin=1in}
\renewcommand{\familydefault}{cmss}
\restylefloat{table}
\restylefloat{figure}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\logit}{\mathrm{logit}}

\begin{document}
\noindent
{\bf BST 140.751 \\ Problem Set 1} \\

\section{Regression and background}
\begin{enumerate}[1.]
\item If your normalize (mean 0 and variance 1) your $Y$ and $X$ vectors,
argue that the regression slope estimate is the correlation.
\item Let $Y$ and $X$ be one dimensional vectors of length $n$. Give
the relationship between the slope from regressing $Y$ on $X$ and $X$
on $Y$.
\item Consider the residuals after mean only regression. Argue
that they sum to 0.
\item Consider the residuals after regression through the origin.
Argue that they are orthogonal to the regressor.
\item Consider the residuals from ordinary linear regression. Argue
that the residuals are orthogonal to both $J_n$ and $X$.
\end{enumerate}

\section{Least squares}
\begin{enumerate}[1.]
\item Show that $I - H$ is an idempotent matrix where $H$ is idempotent.
\item Let $X = [X_1 X_2]$ be an $n\times 2$ design matrix and consider 
$$|| Y - X \beta ||^2$$ where $\beta = (\beta_1 \beta2)'$. Show
that $\hat \beta_2$ can be obtained by taking the residuals after
regressing $X_1$ out of $Y$ and $X_2$ then doing regression through
the origin on the residuals.
\item Argue that $X$, $X'$, $X'X$ and $XX'$ all have the same matrix rank.
\item Suppose that $X$ is such that $X'X = I$. Find the associated least
squares estimate of $\beta$.
\item Suppose that the design matrix is of the form $J_A \otimes I$ where
$J_A$ is a vector of length $A$ and $I$ is a $B\times B$ identity matrix. 
Let $Y$ be a $AB \times 1$ length vector. Find the least squares estimates
associated with this design matrix.
\item Let $X = [X1 X2]$ where $X_1$ is $n\times p_1$ and $X_2$ is $n\times p_2$.
Consider minimizing $|| Y - X \beta ||$ where $\beta = (\beta_1' \beta_2')'$. 
Argue that the least squares estimate of $\beta_1$ can be obtained by 
regressing $e_{Y | X_2} = (I - X_2 (X_2' X_2)^{-1} X_2') Y$ as the outcome
and $e_{X_1 | X_2} = (I - X_1 (X_1 ' X_1)^{-1} X_1')X_2$. as the predictor.
\item Show that if $X = [X_1 \ldots X_p]$ is such that $X'X = I$. Then
the least squares estimate of $\beta$ is $X' Y$ and further $\hat Y$
is $\sum_{j = 1}^p X_i <Y, X_i>$. 
\item Assume that the columns of $X$ have been mean centered so that
$J_n' X = (0 \ldots 0)'$. Suppose further that $X = U D V'$ where $U'U = V'V = I$ and $D$ is a
diagonal matrix of singular values. Argue that the matrix $U = X V' D^{-1}$ results in an orthonormal
basis for the same space as the column space of $X$. Thus, the $\hat Y$ matrix treating
$X$ as the outcome and treating $U$ as the outcome are the same and further
$\hat Y = \sum_{j=1}^p U_i <U_i, Y>$ where $U_i$ are the columns of $U$.
\item If $U=XW$ where $W$ is an invertibble matrix, relate the estimated coefficients
obtained when using $U$ as the design matrix and $W$ as the design matrix.
\end{enumerate}





\section{Computing and analysis}
\begin{enumerate}[1.]
\item Write an R function that takes a $Y$ vector and $X$ matrix and obtains
the least squares fit for the associated linear model.
\item Write and R function that takes an $n\times p$ data matrix, $X$, and
	``whitens'' it via subtracting out a mean and multiplying by a matrix
	so that the resulting matrix has ($p$) sample column means of 0 and
	$p\times p$ sample covariance matrix of $I$.
\end{enumerate}

\end{document}
